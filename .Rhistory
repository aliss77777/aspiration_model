setwd('/Users/alexander.liss/Volumes/possibleshare')
setwd('/NYC-0559/possibleshare/private')
setwd('~/NYC-0559/possibleshare/private')
library(lavaan)
library(semTools)
library(semPlot)
#specify the structural model
piesModel <- " General =~ i1 + i2 + i3
Feature =~ i4 + i5 + i6 + i7
Image =~ i8 + i9 + i10 + i11
PIES =~ General + Feature + Image"
piesDataModel <- " General =~ 0.9*i1 + 0.7*i2 * 0.5*i3
Feature =~ 0.3*i3 + 0.7*i4 + 0.9*i5 + 0.5*i6 + 0.9*i7
Image =~ 0.2*i3 + 0.8*i8 + 0.9*i9 + 0.8*i10 + 0.7*i11
PIES =~ 0.7*General + 0.8*Feature + 0.8*Image"
set.seed(10001)
library(lavaan)
library(semTools)
library(semPlot)
#specify the structural model
piesModel <- " General =~ i1 + i2 + i3
Feature =~ i4 + i5 + i6 + i7
Image =~ i8 + i9 + i10 + i11
PIES =~ General + Feature + Image"
#simulate the data frame
piesDataModel <- " General =~ 0.9*i1 + 0.7*i2 * 0.5*i3
Feature =~ 0.3*i3 + 0.7*i4 + 0.9*i5 + 0.5*i6 + 0.9*i7
Image =~ 0.2*i3 + 0.8*i8 + 0.9*i9 + 0.8*i10 + 0.7*i11
PIES =~ 0.7*General + 0.8*Feature + 0.8*Image"
set.seed(10001)
piesSimData.norm <- simulateData(piesDataModel, sample.nobs = 3600)
print(head(piesSimData.norm), digits = 2)
piesSimData <- data.frame(lapply(piesSimData.norm,
function(x){cut (x, breaks = 7, labels = FALSE)}))
#so, apparently you can use Lapply even though this object is not a list. WOW~!
#detour - i need to understand what that line of code just did
cut(100, breaks = 10, labels = FALSE)
??cut
Z <- stats::rnorm(10000)
table(cut(Z, breaks = -6:6))
hist(Z)
length(Z)
??lapply
x <- list(a = 1:10)
x
lapply(x, mean)
derp <- cut(Z, breaks = 5, labels = FALSE)
hist(derp)
??apply
#resuming the lesson
library(car)
some(piesSimData)
library(psych)
describe(piesSimData)
library(RColorBrewer)
scatterplotMatrix(piesSimData[, c(1, 2, 4, 5, 8, 9)], diagonal = "histogram",
col = brewer.pal(3, "Paired"), ellipse=TRUE)
install.packages("semPlot")
library(semPlot)
#let's do an exploratory factor analysis before we dive in
factanal(piesSimData, factors = 3)
pies.fit <- cfa(piesModel, data = piesSimData)
View(piesSimData.norm)
View(piesSimData.norm)
library(lavaan)
library(semTools)
install.packages("semPlot")
#install.packages("semPlot")
library(semPlot)
#specify the structural model
piesModel <- " General =~ i1 + i2 + i3
Feature =~ i4 + i5 + i6 + i7
Image =~ i8 + i9 + i10 + i11
PIES =~ General + Feature + Image"
#simulate the data frame
piesDataModel <- " General =~ 0.9*i1 + 0.7*i2 * 0.5*i3
Feature =~ 0.3*i3 + 0.7*i4 + 0.9*i5 + 0.5*i6 + 0.9*i7
Image =~ 0.2*i3 + 0.8*i8 + 0.9*i9 + 0.8*i10 + 0.7*i11
PIES =~ 0.7*General + 0.8*Feature + 0.8*Image"
set.seed(10001)
piesSimData.norm <- simulateData(piesDataModel, sample.nobs = 3600)
View(piesSimData.norm)
piesDataModel <- " General =~ 0.9*i1 + 1*i2 * 0.5*i3
Feature =~ 0.3*i3 + 0.7*i4 + 0.9*i5 + 0.5*i6 + 0.9*i7
Image =~ 0.2*i3 + 0.8*i8 + 0.9*i9 + 0.8*i10 + 0.7*i11
PIES =~ 0.7*General + 0.8*Feature + 0.8*Image"
set.seed(10001)
piesSimData.norm <- simulateData(piesDataModel, sample.nobs = 3600)
print(head(piesSimData.norm), digits = 2)
piesDataModel <- " General =~ 0.9*i1 + 0.7*i2 * 0.5*i3
Feature =~ 0.3*i3 + 0.7*i4 + 0.9*i5 + 0.5*i6 + 0.9*i7
Image =~ 0.2*i3 + 0.8*i8 + 0.9*i9 + 0.8*i10 + 0.7*i11
PIES =~ 0.7*General + 0.8*Feature + 0.8*Image"
set.seed(10001)
piesSimData.norm <- simulateData(piesDataModel, sample.nobs = 3600)
print(head(piesSimData.norm), digits = 2)
piesModel <- " General =~ i1 + i2 + i3
Feature =~ i4 + i5 + i6 + i7
Image =~ i8 + i9 + i10 + i11
PIES =~ General + Feature + Image"
piesDataModel <- " General =~ 0.9*i1 + 0.7*i2 * 0.5*i3
Feature =~ 0.3*i3 + 0.7*i4 + 0.9*i5 + 0.5*i6 + 0.9*i7
Image =~ 0.2*i3 + 0.8*i8 + 0.9*i9 + 0.8*i10 + 0.7*i11
PIES =~ 0.7*General + 0.8*Feature + 0.8*Image"
set.seed(10001)
piesSimData.norm <- simulateData(piesDataModel, sample.nobs = 3600)
print(head(piesSimData.norm), digits = 2)
library(lavaan)
library(semTools)
#install.packages("semPlot")
library(semPlot)
#specify the structural model
piesModel <- " General =~ i1 + i2 + i3
Feature =~ i4 + i5 + i6 + i7
Image =~ i8 + i9 + i10 + i11
PIES =~ General + Feature + Image"
#simulate the data frame
piesDataModel <- " General =~ 0.9*i1 + 0.7*i2 + 0.5*i3
Feature =~ 0.3*i3 + 0.7*i4 + 0.9*i5 + 0.5*i6 + 0.9*i7
Image =~ 0.2*i3 + 0.8*i8 + 0.9*i9 + 0.8*i10 + 0.7*i11
PIES =~ 0.7*General + 0.8*Feature + 0.8*Image"
set.seed(10001)
piesSimData.norm <- simulateData(piesDataModel, sample.nobs = 3600)
print(head(piesSimData.norm), digits = 2)
piesSimData <- data.frame(lapply(piesSimData.norm,
function(x){cut (x, breaks = 7, labels = FALSE)}))
#so, apparently you can use Lapply even though this object is not a list. WOW~!
#detour - i need to understand what that line of code just did
cut(100, breaks = 10, labels = FALSE)
??cut
Z <- stats::rnorm(10000)
table(cut(Z, breaks = -6:6))
hist(Z)
length(Z)
??lapply
x <- list(a = 1:10)
x
lapply(x, mean)
derp <- cut(Z, breaks = 5, labels = FALSE)
hist(derp)
??apply
#resuming the lesson
library(car)
some(piesSimData)
library(psych)
describe(piesSimData)
library(RColorBrewer)
scatterplotMatrix(piesSimData[, c(1, 2, 4, 5, 8, 9)], diagonal = "histogram",
col = brewer.pal(3, "Paired"), ellipse=TRUE)
#let's do an exploratory factor analysis before we dive in
factanal(piesSimData, factors = 3)
pies.fit <- cfa(piesModel, data = piesSimData)
scatterplotMatrix(piesSimData[, c(1, 2, 4, 5, 8, 9)], diag = "histogram",
col = brewer.pal(3, "Paired"), ellipse=TRUE)
## setup
library(googleAnalyticsR)
## authenticate, get code from browser pop-up window
ga_auth()
## get your accounts
account_list <- ga_account_list()
#create list of account ID's you want, in this case 132015390
## account_list will have a column called "viewId"
account_list$viewId
## View account_list and pick the viewId you want to extract data from
ga_id <- 132015390
test <- google_analytics(ga_id,
date_range = c("2017-01-01", "2017-01-30"),
metrics = c("ga:sessions", "ga:users", "ga:newUsers", "ga:pageviews", "ga:avgSessionDuration"),
dimensions = "date")
test <- cbind(test, ga_id)
write.csv(account_list, "Fresenius GA properties.csv", row.names = FALSE)
save.image("~/Egnyte/Private/alexander.liss/0_MarSci Practice/Fresenius/GA script v1.RData")
library(WatsonR)
watson.keys.display()
watson.keys.enter()
load("/Users/alexander.liss/Egnyte/Private/alexander.liss/2_clients/VW/brief/modeling/modeling WIP.Rdata")
table(unique(RF.data$VehicleOwned))
table(unique(mydata_withFA_updated$Type))
table((mydata_withFA_updated$Type))
install.packages('IRkernel')
IRkernel::installspec()
IRkernel::installspec(user = FALSE)
library(tidyverse)
library(readxl)
options(digits = 2)
set.seed(42) # for reproducible results
# 1.  Applying Psychographic Model  ------------------------------------------------------
library(caret)
?len
library(utils)
test <- expand.grid(height = seq(60, 80, 5), weight = seq(100, 300, 50),
sex = c("Male","Female"))
View(test)
library(jsonlite)
library(rlist)
library(bigrquery)
get_access_cred()
project_id <- "centrica-pfe"
sql_string <- "SELECT * WIN.digital_market_geo LIMIT 5"
query_exec(sql_string, project = project_id, use_legacy_sql = FALSE)
sql_string <- "SELECT * FROM WIN.digital_market_geo LIMIT 5"
query_exec(sql_string, project = project_id, use_legacy_sql = FALSE)
#__________________________________________________________________________
##### Edit The Code Below ######
# Choose where to save the combined dataset to
setwd('/Users/alexander.liss/Egnyte/Private/alexander.liss/0_MarSci Practice/1_Coding Tutorials/Network Analysis/Scripts for Network Analysis Onboarding')
# Choose the name of the combined dataset
fileToSave = "CombinedDataset.csv" #don't change this
# Choose what Crimson Hexagon Exports need to be combined (just like the format below)
fileNames = c("Posts from 2018-03-01 to 2018-06-11.xlsx",
"Posts from 2017-06-01 to 2017-09-02.xlsx",
"Posts from 2017-09-01 to 2017-12-02.xlsx",
"Posts from 2017-12-01 to 2018-03-02.xlsx")
##### Do Not Edit The Code Below #####
#__________________________________________________________________________
# Check if needed packages are installed
if (("openxlsx" %in% rownames(installed.packages()))==FALSE){
install.packages("openxlsx")
}
if (("plyr" %in% rownames(installed.packages()))==FALSE){
install.packages("plyr")
}
# load packages
library("openxlsx")
library("plyr")
# join Crimson Hexagon Bulk Exports
numFiles = length(fileNames)
for (i in 1:numFiles) {
fileName = fileNames[i]
exportDF = read.xlsx(loadWorkbook(file=fileName))
if (i == 1) {
joinedFile = exportDF
}else{
joinedFile = join(joinedFile,exportDF,type="full")
}
}
write.csv(joinedFile,fileToSave,row.names = FALSE)
setwd("~/Egnyte/Private/alexander.liss/0_MarSci_Practice/1_CodingTutorials/Network Analysis/Scripts for Network Analysis Onboarding")
#__________________________________________________________________________
##### Edit The Code Below ######
# Choose where to save the combined dataset to
setwd('/Users/alexander.liss/Egnyte/Private/alexander.liss/0_MarSci Practice/1_Coding Tutorials/Network Analysis/Scripts for Network Analysis Onboarding')
# Choose the name of the combined dataset
fileToSave = "CombinedDataset.csv" #don't change this
# Choose what Crimson Hexagon Exports need to be combined (just like the format below)
fileNames = c("Posts from 2018-03-01 to 2018-06-11.xlsx",
"Posts from 2017-06-01 to 2017-09-02.xlsx",
"Posts from 2017-09-01 to 2017-12-02.xlsx",
"Posts from 2017-12-01 to 2018-03-02.xlsx")
##### Do Not Edit The Code Below #####
#__________________________________________________________________________
# Check if needed packages are installed
if (("openxlsx" %in% rownames(installed.packages()))==FALSE){
install.packages("openxlsx")
}
if (("plyr" %in% rownames(installed.packages()))==FALSE){
install.packages("plyr")
}
# load packages
library("openxlsx")
library("plyr")
# join Crimson Hexagon Bulk Exports
numFiles = length(fileNames)
for (i in 1:numFiles) {
fileName = fileNames[i]
exportDF = read.xlsx(loadWorkbook(file=fileName))
if (i == 1) {
joinedFile = exportDF
}else{
joinedFile = join(joinedFile,exportDF,type="full")
}
}
write.csv(joinedFile,fileToSave,row.names = FALSE)
# Set Working Directory (This is where the output file will be saved to)
setwd('/Users/alexander.liss/Egnyte/Private/alexander.liss/0_MarSci Practice/1_CodingTutorials/aspiration_model')
# Set Working Directory (This is where the output file will be saved to)
setwd('/Users/alexander.liss/Egnyte/Private/alexander.liss/0_MarSci_Practice/1_CodingTutorials/aspiration_model')
# Choose where to save the combined dataset to
setwd('/Users/alexander.liss/Egnyte/Private/alexander.liss/0_MarSci_Practice/1_CodingTutorials/Network Analysis/Scripts for Network Analysis Onboarding')
# Set Working Directory (This is where the output file will be saved to)
setwd('/Users/alexander.liss/Egnyte/Private/alexander.liss/0_MarSci_Practice/1_CodingTutorials/aspiration_model')
# Set Working Directory (This is where the output file will be saved to)
setwd('/Users/alexander.liss/Egnyte/Private/alexander.liss/0_MarSci_Practice/1_CodingTutorials/aspiration_model')
# Set working directly where you will save the files, and load necessary package
setwd('/Users/alexander.liss/Egnyte/Private/alexander.liss/0_MarSci_Practice/1_CodingTutorials/aspiration_model')
##**************** EDIT BELOW ****************
# Set Working Directory (This is where the output file will be saved to)
setwd('/Users/alexander.liss/Egnyte/Private/alexander.liss/0_MarSci_Practice/1_CodingTutorials/aspiration_model')
# Choose the file to load
#*** IMPORTANT ***
# Make sure the column in your data containing the post message is called "Contents" and
# Make sure the column in your data containing the Screen Name of the Author is called "Author"
# This is Case-Sensitive!
file2Load = "CombinedDataset.csv"
# How many UNIQUE times must a node be mentioned in order to enter the network?
minMentions = 5
##**************** DO NOT EDIT BELOW ****************
#__________________________________________________________________________
# Load the post file
mydata = read.csv(file2Load,
header = TRUE,
stringsAsFactors = FALSE,
fileEncoding="latin1", #on pc comment this part out
sep = ","
)
# Check if needed packages are installed
if (("tm" %in% rownames(installed.packages()))==FALSE){
install.packages("tm")
}
# load "tm" package
library("tm")
#__________________________________________________________________________
## Step 1: Combine all tweets for each individual user (each user has 1 big tweet)
uniqueUsers = unique(mydata$Author)
#uniqueUsers = tolower(uniqueUsers)
numUsers = length(uniqueUsers)
# Mentions And Hashtags
MentionsAndHashtags = matrix("",nrow=numUsers,ncol=1) # Creates empty character array
row.names(MentionsAndHashtags) = uniqueUsers # changes the row names of MentionsAndHashtags to uniqueUsers list
# Only Mentions
OnlyMentions = matrix("",nrow=numUsers,ncol=1) # Creates empty character array
row.names(OnlyMentions) = uniqueUsers # changes the row names of MentionsAndHashtags to uniqueUsers list
# Only Hashtags
OnlyHashtags = matrix("",nrow=numUsers,ncol=1) # Creates empty character array
row.names(OnlyHashtags) = uniqueUsers # changes the row names of MentionsAndHashtags to uniqueUsers list
# Record number of tweets each user has
NumTweets = matrix(0,nrow=numUsers,ncol=1)
for (i in 1:numUsers) {
# find indices of all tweets each user has
userName = uniqueUsers[i] # Screen Name
indexOfTweets = which(mydata$Author==userName) # each index for every "username" tweet
NumTweets[i] = length(indexOfTweets)
# Get each tweet's message
userTweets = mydata$Contents[indexOfTweets] # list of each message "username" said
# Compile each tweet into one big tweet
oneBigTweet = paste(userTweets, sep="", collapse=" ") # collapse list of tweets into "one big tweet"
# Extract @mentions from each tweet
atMentions = regmatches(oneBigTweet, gregexpr('@\\w+', oneBigTweet))[[1]] # get column vector list of all @ mentions
hashtags = regmatches(oneBigTweet, gregexpr('#\\w+', oneBigTweet))[[1]] # get column vector list of all hashtags
# Paste all hashtags and mentions to single character array separated by " "
atMentions = paste(atMentions, collapse = " ")
hashtags = paste(hashtags, collapse = " ")
# Store Targets
MentionsAndHashtags[i] = paste(atMentions,hashtags,collapse=" ")
OnlyMentions[i] = paste(atMentions,collapse=" ")
OnlyHashtags[i] = paste(hashtags,collapse=" ")
}
#__________________________________________________________________________
# Convert each message into mentions and hashtags separated by " "
nRows = nrow(mydata)
Messages = matrix("",nrow=nRows,ncol=1)
for (i in 1:nRows) {
currentMessage = tolower(mydata$Contents[i])
currentHashtags = regmatches(currentMessage, gregexpr('#\\w+', currentMessage))[[1]] # get column vector list of all hashtags
currentMentions = regmatches(currentMessage, gregexpr('@\\w+', currentMessage))[[1]] # get column vector list of all @ mentions
currentMentionsAndHashtags = paste(currentHashtags,currentMentions,collapse=" ")
currentMentionsAndHashtags = paste(" ",currentMentionsAndHashtags," ")
Messages[i] = currentMentionsAndHashtags
}
#__________________________________________________________________________
## Step 2: Make VCorpus and Doc-Term Matrix of @Mentions & HashTags
docs = VCorpus(VectorSource(MentionsAndHashtags))
docsMentions = VCorpus(VectorSource(OnlyMentions))
docsHashtags = VCorpus(VectorSource(OnlyHashtags))
for (i in 1:numUsers) {
docs[[i]]$meta$author = uniqueUsers[i]
docsMentions[[i]]$meta$author = uniqueUsers[i]
docsHashtags[[i]]$meta$author = uniqueUsers[i]
}
#Transform to lower case (need to wrap in content_transformer)
docs <- tm_map(docs,content_transformer(tolower))
docsMentions <- tm_map(docsMentions,content_transformer(tolower))
docsHashtags <- tm_map(docsHashtags,content_transformer(tolower))
#________Reduce the Document-Term Matrix______________
#******* Reduce the Document-Term Matrix to only frequently occuring items
# include only those @mentions that occur in 5 to 156 documents
minThreshold = minMentions
DTMreduced = DocumentTermMatrix(docs,control=list(bounds = list(global = c(minThreshold,1000000))))
DTMreducedMentions = DocumentTermMatrix(docsMentions,control=list(bounds = list(global = c(minThreshold,1000000))))
DTMreducedHashtags = DocumentTermMatrix(docsHashtags,control=list(bounds = list(global = c(minThreshold,1000000))))
#__________________________________________________________________________
## Create Adjacency List
# First turn Document-Term-Matrix into matrix
dtmrMatrix = as.matrix(DTMreduced)
dtmrMatrixMentions = as.matrix(DTMreducedMentions)
dtmrMatrixHashtags = as.matrix(DTMreducedHashtags)
numRows = nrow(dtmrMatrix)
numRowsMentions = nrow(dtmrMatrixMentions)
numRowsHashtags = nrow(dtmrMatrixHashtags)
numCols = ncol(dtmrMatrix)
numColsMentions = ncol(dtmrMatrixMentions)
numColsHashtags = ncol(dtmrMatrixHashtags)
maxRowsForList = numRows*numCols
maxRowsForListMentions = numRowsMentions*numColsMentions
maxRowsForListHashTags = numRowsHashtags*numColsHashtags
numColsForList = 2
AdjacencyList = matrix(nrow=maxRowsForList,ncol=3)
AdjacencyListMentions = matrix(nrow=maxRowsForListMentions,ncol=3)
AdjacencyListHashtags = matrix(nrow=maxRowsForListHashTags,ncol=3)
columnNames = colnames(dtmrMatrix)
columnNamesMentions = colnames(dtmrMatrixMentions)
columnNamesHashtags = colnames(dtmrMatrixHashtags)
listCount = 1
listCountMentions = 1
listCountHashtags = 1
for (i in 1:numRows) {
rowName = uniqueUsers[i]
rowNameMentions = uniqueUsers[i]
rowNameHashtags = uniqueUsers[i]
for (j in 1:numCols) {
elementValue = dtmrMatrix[i,j]
elementValueMentions = tryCatch(dtmrMatrixMentions[i,j], error = function(e) 0)
elementValueHashtags = tryCatch(dtmrMatrixHashtags[i,j], error = function(e) 0)
# Mentions & Hashtags
if (elementValue>0){
colName = columnNames[j]
AdjacencyList[listCount,1] = rowName
AdjacencyList[listCount,2] = colName
AdjacencyList[listCount,3] = elementValue#/NumTweets[i] # Term Frequency Weight
listCount = listCount + 1
}
# Mentions Only
if (elementValueMentions>0){
colName = columnNamesMentions[j]
AdjacencyListMentions[listCountMentions,1] = rowNameMentions
AdjacencyListMentions[listCountMentions,2] = colName
AdjacencyListMentions[listCountMentions,3] = elementValueMentions#/NumTweets[i] # Term Frequency Weight
listCountMentions = listCountMentions + 1
}
# Hashtags Only
if (elementValueHashtags>0){
colName = columnNamesHashtags[j]
AdjacencyListHashtags[listCountHashtags,1] = rowNameHashtags
AdjacencyListHashtags[listCountHashtags,2] = colName
AdjacencyListHashtags[listCountHashtags,3] = elementValueHashtags#/NumTweets[i] # Term Frequency Weight
listCountHashtags = listCountHashtags + 1
}
}
}
# Remove incomplete cases
AdjacencyList = AdjacencyList[complete.cases(AdjacencyList),]
AdjacencyListMentions = AdjacencyListMentions[complete.cases(AdjacencyListMentions),]
AdjacencyListHashtags = AdjacencyListHashtags[complete.cases(AdjacencyListHashtags),]
# Change Column Names in Adjacency List
colnames(AdjacencyList) <- c("Source","Target","Weight");
colnames(AdjacencyListMentions) <- c("Source","Target","Weight");
colnames(AdjacencyListHashtags) <- c("Source","Target","Weight");
# Make Edge List (Adjacency List)
EdgeList = AdjacencyList
# Make Node List (Contains information about nodes)
uniqueSources = unique(EdgeList[,1])
numSources = length(uniqueSources)
uniqueTargets = unique(EdgeList[,2])
numTargets = length(uniqueTargets)
Id = c(uniqueSources,uniqueTargets)
Label = c(uniqueSources,uniqueTargets)
NodeSizeSources = matrix(0,nrow=numSources,ncol=1)
NodeSizeTargets = matrix(0,nrow=numTargets,ncol=1)
# for (i in 1:numTargets) {
#
#   target = uniqueTargets[i]
#   searchStr = paste(" ",target," ")
#   MessagesWithTarget = grepl(searchStr, Messages,ignore.case = TRUE) # logical
#   NodeSizeTargets[i] = mean(na.omit(mydata$InteractionRate[MessagesWithTarget]))
#
#   if (is.nan(NodeSizeTargets[i])==TRUE){
#     NodeSizeTargets[i] = 0
#   }
#
# }
#
# NodeSize = c(NodeSizeSources,NodeSizeTargets)
#
# NodesList = data.frame(Id,Label,NodeSize)
write.csv(AdjacencyList,"AdjListMentionsAndHashtags5min.csv",row.names=FALSE)
# How Many Authors Didn't Make It Into The Reduced Document-Term Matrix?
NumLeftOutPeople = sum(rowSums(dtmrMatrix)==0)
NumLeftOutPeopleMentions = sum(rowSums(dtmrMatrixMentions)==0)
NumLeftOutPeopleHashtags = sum(rowSums(dtmrMatrixHashtags)==0)
